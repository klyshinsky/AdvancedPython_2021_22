{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Знакомство</h2>\n",
    "\n",
    "Меня зовут Клышинский Эдуард Станиславович. <br>\n",
    "В течение этого года мы будем заниматься изучением возможностей языка Питон, а также библиотек для него.<br>\n",
    "Со мной можно связаться по почте eklyshinsky@hse.ru<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Морфологический анализ</h2>\n",
    "\n",
    "На начальных этапах обработки текста проводится два этапа анализа: <b>графематический</b> (выделение предложений и слов) и <b>морфологический</b> (определение начальной формы слова, его части речи и грамматических параметров). Этап синтаксического анализа мы сейчас разбирать не будем, так как его информация требуется не всегда.<br>\n",
    "Задачей графематического анализа является разделение текста на составные части - врезки, абзацы, предложения, слова. В таких задачах как машинный перевод, точность данного этапа может существенно влиять на точность получаемых результатов. Например, точка, используемая для сокращений, может быть воспринята как конец предложения, что полность разорвет его семантику.<br>\n",
    "Задачей морфологического анализа является определение начальной формы слова, его части речи и грамматических параметров. В некоторых случаях от слова требуется только начальная форма, в других - только начальная форма и часть речи.<br>\n",
    "Существует два больших подхода к морфологическому анализу: <b>стемминг</b> и <b>поиск по словарю</b>. Для проведения стемминга оставляется справочник всех окончаний для данного языка. Для пришедшего слова проверяется его окончание и по нему делается прогноз начальной формы и части речи.<br>\n",
    "Например, мы создаем справочник, в котором записываем все окончания прилагательных: <i>-ому, -ему, -ой, -ая, -ий, -ый, ...</i> Теперь все слова, которые имеют такое окончание будут считаться прилагаельными: <i>синий, циклический, красного, больному</i>. Заодно прилагательными будут считаться причастия (<i>делающий, строившему</i>) и местоимения (<i>мой, твой, твоему</i>). Также не понятно что делать со словами, имеющими пустое окончание. Отдельную проблему составляют такие слова, как <i>стекло, больной, вина</i>, которые могут разбираться несколькими вариантами (это явление называется <b>омонимией</b>). Помимо этого, стеммер может просто откусывать окончания, оставляя лишь псевдооснову.<br>\n",
    "Большинство проблем здесь решается, но точность работы бессловарных стеммеров находится на уровне 80%. Чтобы повысить точность испольуют морфологический анализ со словарем. Разработчики составляют словарь слов, встретившихся в текстах (<a href=\"http://opencorpora.org/dict.php\">здесь</a> можно найти пример такого словаря). Теперь каждое слово будет искаться в словаре и не предсказываться, а выдаваться точно. Для слов, отсутствующих в словаре, может применяться предсказание, пообное работе стеммера.<br>\n",
    "Посмотрим как работает словарная морфология на примере системы <a href=\"https://pymorphy2.readthedocs.io/en/latest/\">pymorphy2</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 # Импортируем морфологический анализатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.690476, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),))\n",
      "Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 3),))\n",
      "Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.023809, methods_stack=((DictionaryAnalyzer(), 'стекло', 1015, 3),))\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "wordform = morph.parse('стекло')  # Проведем анализ слова \"стекло\".\n",
    "for w in wordform: # Возвращается список вариантов разбора.\n",
    "    print(w) # Посмотрим на полученный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "стекло\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "wordform = morph.parse('стекло')  # Проведем анализ слова \"стекло\".\n",
    "for w in wordform: # Возвращается список вариантов разбора.\n",
    "    if {'NOUN', 'accs'} in w.tag: # Возьмем только варианты разбора как существительного.\n",
    "        print(w.normal_form)# Посмотрим на начальные формы в анализе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='делавшихся', tag=OpencorporaTag('PRTF,impf,intr,past,actv plur,gent'), normal_form='делаться', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'делавшихся', 234, 61),)),\n",
       " Parse(word='делавшихся', tag=OpencorporaTag('PRTF,impf,intr,past,actv anim,plur,accs'), normal_form='делаться', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'делавшихся', 234, 63),)),\n",
       " Parse(word='делавшихся', tag=OpencorporaTag('PRTF,impf,intr,past,actv plur,loct'), normal_form='делаться', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'делавшихся', 234, 66),))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('делавшихся')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='варкалось', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='варкалось', score=0.5000531180282588, methods_stack=((DictionaryAnalyzer(), 'лось', 123, 0), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'варка'))),\n",
       " Parse(word='варкалось', tag=OpencorporaTag('VERB,impf,intr neut,sing,past,indc'), normal_form='варкаться', score=0.4999468819717412, methods_stack=((FakeDictionary(), 'варкалось', 234, 9), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), 'алось')))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('варкалось')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из вывода, слово \"стекло\" может быть неодушевленным существительным среднего рода, единственного числа, именительного падежа <i>tag=OpencorporaTag('NOUN,inan,neut sing,nomn')</i>, аналогично, но в винительном падеже (<i>'NOUN,inan,neut sing,accs'</i>), и глаголом <i>'VERB,perf,intr neut,sing,past,indc'</i>. При этом в первой форме оно встречается в 75% случаев (<i>score=0.75</i>), во второй в 18,75% случаев (<i>score=0.1875</i>), а как глагол - лишь в 6,25% (<i>score=0.0625</i>). Самым простым видом борьбы с омонимией является выбор нулевого элемента из списка, возвращенного морфологическим анализом. Такой подход дает около 90% точности при выборе начальной формы и до 80% если мы обращаем внимание на грамматические параметры.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо Pymorphy можно использовать PyMystem. Его плюсом является тот факт, что он сам проводит графематический анализ и снимает омонимию. Используя функцию lemmatize можно получить набор начальных форм слов. Используя функцию analyze можно получить полную информацию о словах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `lemmatize` делит текст на слова и знаки препинания, а затем возвращает для них только начальную форму.\n",
    "\n",
    "Функция `analyze` возвращает не только начальную форму, но и всю информацию о слове, как это делал перед этим Pymorphy. \n",
    "\n",
    "Основным отличием является то, что Mystem снимает омонимию. Как видно из примера, делает он это не всегда корректно, но нам не придется думать о том, какое вариант разбора следует взять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['этот', ' ', 'тип', ' ', 'становиться', ' ', 'есть', ' ', 'в', ' ', 'цех', '\\n']\n",
      "-----\n",
      "{'analysis': [{'lex': 'этот', 'wt': 1, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], 'text': 'эти'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'тип', 'wt': 0.8700298642, 'gr': 'S,муж,неод=(вин,мн|им,мн)'}], 'text': 'типы'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'становиться', 'wt': 0.9821285244, 'gr': 'V,нп=прош,мн,изъяв,сов'}], 'text': 'стали'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'есть', 'wt': 0.0492236161, 'gr': 'V,несов,пе=инф'}], 'text': 'есть'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'цех', 'wt': 1, 'gr': 'S,муж,неод=(дат,ед|местн,ед)'}], 'text': 'цеху'}\n",
      "{'text': '\\n'}\n"
     ]
    }
   ],
   "source": [
    "phrase = 'эти типы стали есть в цеху'\n",
    "mystem = pymystem3.Mystem()\n",
    "print(mystem.lemmatize(phrase)) # lemmatize возвращает только начальные формы.\n",
    "print('-----')\n",
    "for word in mystem.analyze(phrase): # analyze возвращает полный разбор.\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'система', 'wt': 1, 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'Системы'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'автоматизированный', 'wt': 0.6585352169, 'gr': 'A,полн=(вин,ед,муж,од|род,ед,муж|род,ед,сред)'}], 'text': 'автоматизированного'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'проектирование', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'проектирования'}\n",
      "{'text': '\\n'}\n",
      "-----\n",
      "{'analysis': [{'lex': 'система', 'wt': 1, 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'Системы'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'сделать', 'wt': 1, 'gr': 'V,сов,пе=(прош,вин,ед,прич,полн,муж,страд,од|прош,род,ед,прич,полн,муж,страд|прош,род,ед,прич,полн,сред,страд)'}], 'text': 'сделанного'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'проектирование', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'проектирования'}\n",
      "{'text': '\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на варианты разбора фраз\n",
    "phrase = 'Системы автоматизированного проектирования' # ... и обнаружим ошибку.\n",
    "for word in mystem.analyze(phrase): \n",
    "    print(word)\n",
    "print('-----')\n",
    "phrase = 'Системы сделанного проектирования' \n",
    "for word in mystem.analyze(phrase): \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одна библиотека - NLTK. По сравнению с двумя предыдущими библиотеками она обладает более широкой функциональностью и изначально писалась для работы с разными языками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Иностранный морфологический анализатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом использования необходимо загрузить необходимые библиотеки или корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [*] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [*] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: q\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # По дороге будут появляться поле ввода. Грузит всё из Сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/edward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/edward/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     /home/edward/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/edward/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # Сразу грузит что попросили.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_ru')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `word_tokenize` возвращает начальные формы слов. \n",
    "\n",
    "Функция `pos_tag` возвращает список начальных форм и их частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Эти', 'типы', 'стали', 'есть', 'в', 'цеху'],\n",
       " [('Эти', 'типы'),\n",
       "  ('типы', 'стали'),\n",
       "  ('стали', 'есть'),\n",
       "  ('есть', 'в'),\n",
       "  ('в', 'цеху')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Токенизация. Надо не забыть сказать, что анализируем русский язык.\n",
    "tokens = nltk.word_tokenize('Эти типы стали есть в цеху', language='russian') \n",
    "bi_tokens = list(nltk.bigrams(tokens))\n",
    "tokens, bi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Эти', 'A-PRO=pl'),\n",
       "  ('типы', 'S'),\n",
       "  ('стали', 'V'),\n",
       "  ('есть', 'V'),\n",
       "  ('в', 'PR'),\n",
       "  ('цеху', 'S')],\n",
       " [(('Эти', 'A-PRO=pl'), ('типы', 'S')),\n",
       "  (('типы', 'S'), ('стали', 'V')),\n",
       "  (('стали', 'V'), ('есть', 'V')),\n",
       "  (('есть', 'V'), ('в', 'PR')),\n",
       "  (('в', 'PR'), ('цеху', 'S'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = nltk.pos_tag(tokens, lang='rus') # Частеречная разметка.\n",
    "bi_pos = list(nltk.bigrams(pos))\n",
    "pos, bi_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У NLTK заведен список стоп-слов, которые лучше фильтровать при анализе текстов. Но их не очень много. Зато самые мешающиеся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего русских стоп-слов 151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Эти', 'типы', 'стали', 'цеху']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оставим только те слова, которых нет в списке стоп-слов.\n",
    "filtered_words = [token for token in tokens \n",
    "                  if token not in nltk.corpus.stopwords.words('russian')]\n",
    "print('всего русских стоп-слов', len(nltk.corpus.stopwords.words('russian')))\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведены примеры функций морфологического анализа текста для разных библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Подгружаем библиотеку с регулярными выражениями.\n",
    "\n",
    "# Для определения типов параметров функций нам потребуется простой питоновский ...\n",
    "from typing import Optional\n",
    "from pymorphy2.analyzer import MorphAnalyzer\n",
    "from pymystem3.mystem import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12z12', '12')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(([0-9]+)z\\2)','asdf123 asd3213 sdfd 4324 fsdfsd 543 34 12z12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['милый_ADJF', 'мама_NOUN', 'мыло_NOUN', 'белый_ADJF', 'рама_NOUN']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pymorphy\n",
    "def normalizePymorphy(morph: MorphAnalyzer, text: str) -> list:\n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        words.append(f'{pv[0].normal_form}_{str(pv[0].tag.POS)}') # Берем наиболее вероятную форму.\n",
    "    return words    \n",
    "        \n",
    "# Обратите внимание, что про иностранные слова словарь ничего не знает.\n",
    "normalizePymorphy(morph, 'Милая мама мыла белую раму.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['милый_A', 'мама_S', 'мыло_S', 'белый_A', 'рама_S']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyMystem\n",
    "def normalizePymystem(mystem: Mystem, text: str) -> list:\n",
    "    tokens = mystem.analyze(text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if 'analysis' in t.keys():\n",
    "            if t['analysis'] != []:\n",
    "                words.append(f\"{t['analysis'][0]['lex']}_{t['analysis'][0]['gr'][0]}\")\n",
    "            else:\n",
    "                words.append(f\"{t['text']}_U\")\n",
    "    return words    \n",
    "        \n",
    "# Не все считают, что причастие всегда выступает в роли глагола, но иногда так значительно проще.\n",
    "normalizePymystem(mystem, \"Милая мама мыла белую раму.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Милая_A=f', 'мама_S', 'мыла_V', 'белую_A=f', 'раму_S', '._NONLEX']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "def normalizeNLTK(text):\n",
    "    tokens = nltk.pos_tag(nltk.word_tokenize(text), lang='rus')\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if t[0] != t[1]:\n",
    "            words.append(f'{t[0]}_{t[1]}')\n",
    "    return words    \n",
    "        \n",
    "# А вот здесь с частеречной разметкой всё плохо, а параметров нет вовсе.\n",
    "normalizeNLTK(\"Милая мама мыла белую раму.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы будем использовать pymorphy, так как он немного пошустрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим с какой скоростью работают эти анализаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/war_and_peace.txt') as fil:\n",
    "    textWP = fil.read()\n",
    "# Выделяем все слова написанные русской кириллицей.\n",
    "words = [w[0] for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "newtext = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n            Лев Николаевич Толстой. Война и мир. Том 1\\n\\n             * ЧАСТЬ ПЕРВАЯ. * \\n\\n\\n          \\n          \\n\\n            I.\\n\\n\\n          \\n               -- Еh bien, mon prince. Gênes et Lucques ne sont plus que des apanages,\\n          des поместья, de la famille Buonaparte.  Non, je  vous préviens, que si vous\\n          ne  me dites pas, que nous avons la guerre, si vous vous permettez encore de\\n          pallier  toutes les infamies, toutes les  atrocités  de cet  Antichrist  (ma\\n          parole, j'y  crois) -- je  ne  vous  connais plus, vous n'êtes plus mon ami,\\n          vous n'êtes  plus  мой  верный  раб,  comme  vous  dites.  [1]  Ну,\\n          здравствуйте, здравствуйте.  Je vois  que  je  vous fais  peur, [2]\\n          садитесь и рассказывайте.\\n               Так говорила в и\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textWP[:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Лев', ''),\n",
       " ('Николаевич', ''),\n",
       " ('Толстой', ''),\n",
       " ('Война', ''),\n",
       " ('и', ''),\n",
       " ('мир', ''),\n",
       " ('Том', ''),\n",
       " ('ЧАСТЬ', ''),\n",
       " ('ПЕРВАЯ', ''),\n",
       " ('Е', '')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445508\n"
     ]
    }
   ],
   "source": [
    "print(len(words)) # Вся \"Война и мир\" занимает примерно 450 000 слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.45 s, sys: 0 ns, total: 6.45 s\n",
      "Wall time: 6.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniMorphy = []\n",
    "for w in words:\n",
    "    r = morph.parse(w)\n",
    "    iniMorphy.append(r[0].normal_form) # Берем только начальные формы слов, остальное нам не надо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может быть это всё долгая работа списка?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.26 s, sys: 0 ns, total: 7.26 s\n",
      "Wall time: 7.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniMorphy = []\n",
    "for w in words:\n",
    "    r = morph.parse(w)\n",
    "#     iniMorphy.append(r[0].normal_form) # Берем только начальные формы слов, остальное нам не надо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на скорость работы PyMyStem.\n",
    "\n",
    "!!! Внимание, высокое потребление памяти!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 29s, sys: 14 s, total: 2min 43s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniStem = mystem.lemmatize(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "iniStem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 11.2 s, total: 1min 40s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniStem = mystem.analyze(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Борьба за производительность</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примерно 50 000 слов в секунду для PyMorphy и 25 000 (со снятием омонимии) для MyStem.<br>\n",
    "Хорошо, но хочется быстрее.<br>\n",
    "Давайте посмотрим на статистику встречаемости слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # Импортируем счетчик из стандартных библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и : 20324\n",
      "в : 10202\n",
      "не : 8434\n",
      "что : 7298\n",
      "на : 6441\n",
      "он : 5986\n",
      "с : 5748\n",
      "его : 3864\n",
      "как : 3681\n",
      "к : 3408\n"
     ]
    }
   ],
   "source": [
    "frqs = Counter(words) # Считаем сколько раз встречается каждое слово, ...\n",
    "frqsSorted = sorted(frqs.items(), key=lambda x: x[1], reverse=True) # ... сортируем, ...\n",
    "for x in frqsSorted[:10]: # ... и выводим 10 самых частоных слов.\n",
    "    print(f'{x[0]} : {frqs[x[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16921357192238973\n",
      "0.3676791438088654\n"
     ]
    }
   ],
   "source": [
    "print(sum([x[1] for x in frqsSorted[:10]]) / len(words))\n",
    "print(sum([x[1] for x in frqsSorted[:100]]) / len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сто самых частотных словоформ занимает примерно треть текста!<br>\n",
    "Возможно, если мы сумеем кешировать результаты работы морфологического анализатора, он начнет работать быстрее. Используем для этого обычный питоновский словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:31<00:00, 320.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10000)):\n",
    "    sleep(0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 852 ms, sys: 0 ns, total: 852 ms\n",
      "Wall time: 856 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniMorphy2 = []\n",
    "cash = {} # Словарь для кеширования.\n",
    "for w in words:\n",
    "    if w in cash.keys(): # Если слово было закешировано, возьмем его из словаря.\n",
    "        iniMorphy2.append(cash[w])\n",
    "    else: # В противном случае проведем морфологический анализ.\n",
    "        r = morph.parse(w)\n",
    "        iniMorphy2.append(r[0].normal_form)\n",
    "        cash[w] = r[0].normal_form \n",
    "    # Вообще-то, можно было кешировать все результаты. Но нам же нужна только наиболее вероятная начальная форма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорение почти в 8 раз!<br>\n",
    "Имеет смысл сделать из этого какую-то удобную обертку, чтобы повторно использовать в своих дальнейших разработках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание и использование классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим свой собственный класс.<br>\n",
    "Класс - это тип, определенный пользователем (программистом). Класс содержит в себе как данные, так и функции, которые работают с этими данными.<br>\n",
    "Так как это тип, то можно создавать переменные этого типа. Каждая переменная будет хранить и обрабатывать свой набор данных.<br>\n",
    "В каждую функцию класса обязательно передается переменная, которая обычно называется self. Эта переменная содержит в себе объект, для которого производится вызов функции. Мы можем обращаться к свойствам данного объекта, используя или модифицируя тем самым этот объект."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ключевое слово class после которого идет название нашего класса.\n",
    "class FasterMorphology:\n",
    "    \"\"\"Class description.\"\"\"\n",
    "    \n",
    "    __morpho: MorphAnalyzer\n",
    "    __cash: dict\n",
    "    \n",
    "    def __init__(self): # Функция инициализации объекта после его создания.\n",
    "        \"\"\"Initiolizes an object\"\"\"\n",
    "        # Создаем новую морфологию в каждом объекте. \n",
    "        # А вдруг мы будем потом работать с разными языками? У каждого объекта должна быть своя.\n",
    "        self.__morpho = MorphAnalyzer() \n",
    "        self.__cash = {} # Создаем словарь для кеширования.\n",
    "        \n",
    "    def analyzeWords(self, words: list) -> list:\n",
    "        \"\"\" Multiline comment after a function will be placed into its documentation.\n",
    "            The function's documentation is accessible by Shift+Tab.\n",
    "            \n",
    "            This function analyses a list of tokens using pymorphy2 and hashes result for faster processing.\n",
    "            \n",
    "            words - list of words.\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w in self.__cash:\n",
    "                res.append(self.__cash[w])\n",
    "            else:\n",
    "                r = self.__morpho.parse(w)[0].normal_form\n",
    "                res.append(r)\n",
    "                self.__cash[w] = r\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания объекта необходимо вызвать функцию с тем же именем, что и имя его типа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = FasterMorphology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FasterMorphology' object has no attribute '__morpho'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70451/4234219641.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__morpho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'FasterMorphology' object has no attribute '__morpho'"
     ]
    }
   ],
   "source": [
    "fast.__morpho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть переменная нужного типа и мы можем обращаться к ее полям, а также вызывать ее методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 755 ms, sys: 3.31 ms, total: 759 ms\n",
      "Wall time: 757 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = fast.analyzeWords(words) # Нажмите Shift+Tab чтобы посмотреть документацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лев',\n",
       " 'николаевич',\n",
       " 'толстой',\n",
       " 'война',\n",
       " 'и',\n",
       " 'мир',\n",
       " 'тот',\n",
       " 'часть',\n",
       " 'первый',\n",
       " 'быть']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь еще раз то же самое, но с заполненным кешем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73.2 ms, sys: 422 µs, total: 73.6 ms\n",
      "Wall time: 73.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = fast.analyzeWords(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорение еще в 10 раз!<br>\n",
    "Правда, заодно мы выяснили, что слова надо приводить к единому написанию, устраняя, например, заглавные буквы. Повторим эксперимент без них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = [w[0].lower() for w in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", textWP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast2 = FasterMorphology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 714 ms, sys: 7.16 ms, total: 721 ms\n",
      "Wall time: 720 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = fast2.analyzeWords(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем следующую модификацию нашего класса. Мы хотим, чтобы пользователь мог выбирать с какой системой морфологического анализа работать - PyMorphy или MyStem. Для этого мы создадим две функции каждая из которых отвечает за свою систему. Обе эти функции будут обеспечивать унифицированный интерфейс, принимая на вход список токенов и выдавая список начальных форм. Вообще-то следовало бы передавать на вход строку с текстом, чтобы в случае PyMorphy самим проводить токенизацию, но нам отчего-то захотелось сохранить предыдущий интерфейс (унаследованная система?). \n",
    "\n",
    "В конструкторе (функция `__init__`) мы создадим объект морфологии нужного типа и сохраним в свойство класса `self.analyzeWords` функцию анализа, соответствующую переданному пользователем парамету. Теперь для проведения морфологического анализа необходимо вызвать `self.analyzeWords`, но при этом соверешнно не обязательно знать каким образом был проинициализирован объект нашего класса и какую систему морфологического анализа он использует. Унифицированный интерфейс позволяет пользователю не задумываться над этими вопросами, просто получая результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ключевое слово class после которого идет название нашего класса.\n",
    "class FasterMorphologyUnified:\n",
    "    \n",
    "    def __init__(self, dict_type: str): # Функция инициализации объекта после его создания.\n",
    "        # Создаем новую морфологию в каждом объекте. \n",
    "        # А вдруг мы будем потом работать с разными языками? У каждого объекта должна быть своя.\n",
    "        if dict_type == 'PyM':\n",
    "            self.morpho = pymorphy2.MorphAnalyzer() \n",
    "            self.cash = {} # Создаем словарь для кеширования.\n",
    "            self.analyzeWords = self.analyzeWordsWithPymorphy\n",
    "        elif dict_type == 'MyS':\n",
    "            self.mystem = pymystem3.Mystem()\n",
    "            self.analyzeWords = self.analyzeWordsWithMystem\n",
    "        # Вообще-то надо предусмотреть вариант, если нам передали какое-то еще значение, которого мы не знаем.\n",
    "        self.mode = dict_type # Сохраним, чтобы потом можно было понять что за словарь использовался.\n",
    "            \n",
    "    def analyzeWordsWithMystem(self, words: list) -> list:\n",
    "        \"\"\" Функция анализа при помощи MyStem.\n",
    "            words - список токенов для анализа.\n",
    "        \"\"\"\n",
    "        text = ' '.join(words)\n",
    "        tokens = self.mystem.analyze(text)\n",
    "        res = []\n",
    "        for t in tokens:\n",
    "            if 'analysis' in t.keys():\n",
    "                if t['analysis'] != []:\n",
    "                    res.append(t['analysis'][0]['lex'] + '_' + t['analysis'][0]['gr'][0])\n",
    "                else:\n",
    "                    res.append(t['text'] + '_' + 'U')\n",
    "        return res    \n",
    "\n",
    "    def analyzeWordsWithPymorphy(self, words: list) -> list:\n",
    "        \"\"\" Функция анализа при помощи PyMorphy.\n",
    "            words - список токенов для анализа.\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w in self.cash:\n",
    "                res.append(self.cash[w])\n",
    "            else:\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                res.append(r)\n",
    "                self.cash[w] = r\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fast3 = FasterMorphologyUnified('PyM')#('MyS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 14.3 s, total: 2min 22s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res=fast3.analyzeWords(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Векторизация текстов</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем класс, отвечающий за векторизацию текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для определения меры сходства двух текстов используется косинусная мера сходства, рассчитываемая по следующей формуле: $cos(a,b)=\\frac{\\sum{a_i * b_i}}{\\sqrt {\\sum{a_i^2}*\\sum{b_i^2}}}$.<br>\n",
    "Вообще-то, использовать стандартную функцию рассчета косинусной меры сходства из <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\">sklearn</a> было бы быстрее. Но мне хотелось показать как работать с разными типами входа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Другое имя класса, так как он обладает несколько иной функциональностью.\n",
    "class FasterMorphology2:\n",
    "    \"\"\" Класс для быстрого морфологического анализа текстов и их векторизации.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self): # Функция инициализации объекта после его создания.\n",
    "        self.morpho = pymorphy2.MorphAnalyzer()\n",
    "        self.__cash = {}\n",
    "        self.__dictionary = {} # Добавим словарь для запоминания, на каком месте вектора находится какая начальная форма.\n",
    "        \n",
    "    def analyzeWords(self, words: list) -> list:\n",
    "        \"\"\" Проводит морфологический анализ списка токенов words.\n",
    "            Возвращает список начальных форм слов.\n",
    "        \"\"\"\n",
    "        res: list\n",
    "            \n",
    "        res=[]\n",
    "        for w in words:\n",
    "            if w in self.__cash: # Сперва ищем очередное слово в кеше.\n",
    "                res.append(self.__cash[w])\n",
    "            else: # Если его там нет, проводим морфологический анализ и кешируем.\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                res.append(r)\n",
    "                self.__cash[w] = r\n",
    "                if r not in self.__dictionary: # Также для каждой начальной формы запоминаем ее позицию в векторе.\n",
    "                    self.dictionary[r] = len(self.dictionary) + 1\n",
    "        return res\n",
    "    \n",
    "    def analyzeText(self, text: str) -> list:\n",
    "        \"\"\" Проводит морфологический анализ строки с текстом text. \n",
    "            Выделяет из нее слова, написанные русской кириллицей.\n",
    "            Возвращает список начальных форм слов.\n",
    "        \"\"\"\n",
    "        words: list\n",
    "        \n",
    "        words = [w[0] for w in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        return self.analyzeWords(words)\n",
    "        \n",
    "    # Вообще-то тоже самое умеет Counter, но ему надо сперва привести слова к начальной форме.\n",
    "    def vectorizeAsDict(self, words) -> dict:\n",
    "        \"\"\" Возвращает векторное разреженное представление текста в виде словаря.\n",
    "            Текст передается как список токенов words.\n",
    "            Вместо позиции для индексации используется само слово.\n",
    "            Возвращает словарь с начальными формами в ключах и частотами этих форм.\n",
    "        \"\"\"\n",
    "        vct: dict\n",
    "        res: list\n",
    "        \n",
    "        vct = {}\n",
    "        res = []\n",
    "        for w in words: # Для каждого слова проводим анализ.\n",
    "            if w in self.__cash:\n",
    "                vct[self.__cash[w]] = vct.get(self.__cash[w], 0) + 1 # Считаем частоты слов.\n",
    "            else:\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                res.append(r)\n",
    "                self.__cash[w] = r\n",
    "                vct[r] = vct.get(r, 0) + 1\n",
    "                if r not in self.__dictionary:\n",
    "                    self.__dictionary[r] = len(self.__dictionary)\n",
    "        return vct\n",
    "    \n",
    "    def clearDict(self):\n",
    "        \"\"\" Очищает словарь. Вдруг надо пересчитать так как изменилась размерность пространства.\n",
    "        \"\"\"\n",
    "        self.dictionary = {}\n",
    "    \n",
    "    def formDict(self, texts: list):\n",
    "        \"\"\" Сформировать словарь по тексту не формируя разметку текста.\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                if word not in self.cash:\n",
    "                    r = self.morpho.parse(w)[0].normal_form\n",
    "                    self.__cash[word] = r\n",
    "                    if r not in self.__dictionary:\n",
    "                        self.__dictionary[r] = len(self.__dictionary)\n",
    "    \n",
    "    def vectorizeAsList(self, words: list) -> list:\n",
    "        \"\"\" Возвращает векторное представление текста в виде плотного списка (включает нули).\n",
    "            Текст передается как список токенов words.\n",
    "            Позиция каждого слова в векторе определяется числом, хранимым в dictionary.\n",
    "            Возвращает словарь с начальными формами в ключах и частотами этих форм.\n",
    "        \"\"\"\n",
    "        # Сперва обновляем dictionary.\n",
    "        for word in words:\n",
    "            if word not in self.__cash:\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                self.__cash[word] = r\n",
    "                if r not in self.__dictionary:\n",
    "                    self.__dictionary[r]=len(self.__dictionary.keys())\n",
    "        # Теперь, когда все слова есть в кеше и словаре и известен размер вектора, можно приступать к векторизации.\n",
    "        vct = [0 for _ in self.__dictionary]\n",
    "        for word in words:\n",
    "            vct[self.__dictionary[self.__cash[word]]] += 1\n",
    "        return vct\n",
    "    \n",
    "    def vectorizeAsList2(self, words: list) -> list:\n",
    "        \"\"\" Возвращает векторное представление текста в виде плотного списка (включает нули).\n",
    "            Текст передается как список токенов words. В вектор включаются только слова, находящиес в словаре.\n",
    "            Позиция каждого слова в векторе определяется числом, хранимым в dictionary.\n",
    "            Возвращает словарь с начальными формами в ключах и частотами этих форм.\n",
    "        \"\"\"\n",
    "        vct = [0 for _ in self.__dictionary]\n",
    "        for word in words:\n",
    "            if word in self.__cash:\n",
    "                vct[self.__dictionary[self.__cash[word]]] += 1\n",
    "        return vct\n",
    "\n",
    "    def vectorizeAsArray(self, words: list) ->list:\n",
    "        \"\"\" Возвращает векторное представление текста в виде плотного массива (включает нули).\n",
    "            Текст передается как список токенов words.\n",
    "            Позиция каждого слова в векторе определяется числом, хранимым в dictionary.\n",
    "            Возвращает словарь с начальными формами в ключах и частотами этих форм.\n",
    "        \"\"\"\n",
    "        # Сперва обновляем dictionary.\n",
    "        for word in words:\n",
    "            if word not in self.__cash:\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                self.cash[word] = r\n",
    "                if r not in self.__dictionary:\n",
    "                    self.__dictionary[r]=len(self.__dictionary.keys())\n",
    "        # Теперь, когда все слова есть в кеше и словаре и известен размер вектора, можно приступать к векторизации.\n",
    "        vct = np.zeros((len(self.__dictionary)))\n",
    "        for word in words:\n",
    "            vct[self.__dictionary[self.__cash[word]]] += 1\n",
    "        return vct\n",
    "\n",
    "    # Здесь мы заложили проблему. Функция не умеет считать расстояние между np.array.\n",
    "    # А ещё эти две функции не имеют никакого отношения к морфологическому анализу.\n",
    "    # Но представим себе, что мы завели ещё две функции, которые считают расстояние между текстами, а не векторами.\n",
    "    def cosineSimilarity(self, a, b) -> float:\n",
    "        \"\"\" Функция расчета косинусной меры сходства между двумя векторными представлениями текста.\n",
    "            Работает по-разному в зависимости от представления вектора.\n",
    "        \"\"\"\n",
    "        if type(a) != type(b): # Тип векторов должен совпадать.\n",
    "            return None\n",
    "        if isinstance(a, list): # Если это списки, значит это плотное представление вектора.\n",
    "            if 0 == len(a) or 0 == len(b) or len(a) != len(b): # Длины векторов в этом случае должны совпадать.\n",
    "                return 0\n",
    "            sumab = sum([a[na] * b[na] for na in range(len(a))])\n",
    "            suma2 = sum([a[na] * a[na] for na in range(len(a))])\n",
    "            sumb2 = sum([b[na] * b[na] for na in range(len(a))])\n",
    "            return sumab / math.sqrt(suma2 * sumb2)        \n",
    "        elif isinstance(a, dict): # Разреженное представление вектора - хранятся только ненулевые значения.\n",
    "            if 0 == len(a.keys()) or 0 == len(b.keys()): # Вектора должны хранить хоть что-то.\n",
    "                return 0\n",
    "            sumab = sum([a[na] * b[na] for na in set(a.keys()) & set(b.keys())])\n",
    "#            sumab=sum([a[na]*b[na] for na in a.keys() if na in b.keys()])\n",
    "            suma2 = sum([a[na] * a[na] for na in a.keys()])\n",
    "            sumb2 = sum([b[nb] * b[nb] for nb in b.keys()])\n",
    "            return sumab / math.sqrt(suma2 * sumb2)  \n",
    "        return 0\n",
    "    \n",
    "    def JaccardCoefficient(self, a, b) -> float:\n",
    "        \"\"\" Коэффициент Жаккара - отношение количества слов, встречающихся в обоих текстах к объединению лексики.\n",
    "        \"\"\"\n",
    "        if type(a) != type(b): # Тип векторов должен совпадать.\n",
    "            return None\n",
    "        if isinstance(a, list): # Если это списки, значит это плотное представление вектора.\n",
    "            if 0 == len(a) or 0 == len(b) or len(a) != len(b): # Длины векторов в этом случае должны совпадать.\n",
    "                return 0\n",
    "            union = len(a) - [aa * bb for aa, bb in zip(a, b)].count(0)\n",
    "            intersection = len(a) - [aa + bb for aa, bb in zip(a, b)].count(0)\n",
    "            return union / intersection\n",
    "        elif isinstance(a, dict): # Разреженное представление вектора - хранятся только ненулевые значения.\n",
    "            if 0 == len(a.keys()) or 0 == len(b.keys()): # Вектора должны хранить хоть что-то.\n",
    "                return 0\n",
    "            return len(set(a.keys()) & set(b.keys())) / len(set(a.keys()) | set(b.keys()))\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как работает новая морфология. Возьмем также один из \"Севастопольских рассказов\" того же автора, чтобы было что использовать при расчете косинусной меры сходства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster3 = FasterMorphology2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sebastopol.txt') as fil:\n",
    "    textWP = fil.read()\n",
    "words3 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "newtext3 = ' '.join(words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим плотное и разреженное представление для двух текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd1 = faster3.vectorizeAsDict(words)\n",
    "vd2 = faster3.vectorizeAsDict(words3)\n",
    "vl1 = faster3.vectorizeAsList(words)\n",
    "vl2 = faster3.vectorizeAsList(words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как быстро считается разреженное и плотное предствления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.34 µs\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.58 µs\n",
      "0.9561367711772639 0.9561367711772639\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "r1 = faster3.cosineSimilarity(vd1, vd2)\n",
    "%time\n",
    "r2 = faster3.cosineSimilarity(vl1, vl2)\n",
    "print(r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529 15350 18840\n"
     ]
    }
   ],
   "source": [
    "print(vl1.count(0), vl2.count(0), len(vl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.53 µs\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.53 µs\n",
      "0.1571656050955414 0.1571656050955414\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "r1 = faster3.JaccardCoefficient(vd1, vd2)\n",
    "%time\n",
    "r2 = faster3.JaccardCoefficient(vl1, vl2)\n",
    "print(r1, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько наши вектора занимают памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589920\n",
      "153752\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.getsizeof(vd1))\n",
    "print(sys.getsizeof(vl1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница по памяти в 4 раза. Попробуем с numpy.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150824\n"
     ]
    }
   ],
   "source": [
    "va1 = faster3.vectorizeAsArray(words)\n",
    "print(sys.getsizeof(va1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь какой-то небольшой выигрыш. Мораль - проще разработать свой собственный класс для разреженного хранения на основе общего для всех словаря с индексом и двумя массивами для индекса слов конкретного текста и их частот.\n",
    "\n",
    "Хорошо, продолжим с косинусной мерой. Попробуем посчитать сходство с \"Марсианином\" Энди Вейра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/veyr/index_split_017.xhtml') as fil: # Грузим главу 17, она побольше.\n",
    "    textM17 = fil.read()\n",
    "words4 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textM17)]\n",
    "newtext4 = ' '.join(words4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считаем вектора.\n",
    "vd3 = faster3.vectorizeAsDict(words4)\n",
    "vl3 = faster3.vectorizeAsList(words4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7853540382103844\n",
      "0.7776684489226509\n",
      "CPU times: user 13 ms, sys: 167 µs, total: 13.2 ms\n",
      "Wall time: 12.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vd1, vd3))\n",
    "print(faster3.cosineSimilarity(vd2, vd3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "CPU times: user 386 µs, sys: 0 ns, total: 386 µs\n",
      "Wall time: 256 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vl1, vl3))\n",
    "print(faster3.cosineSimilarity(vl2, vl3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то пошло не так. Постараемся понять что именно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52011/5167731.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msumab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msuma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msumb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msumab\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuma2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msumb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52011/5167731.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msumab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msuma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msumb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msumab\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuma2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msumb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sumab = sum([vl1[na] * vl3[na] for na in range(len(vl3))])\n",
    "suma2 = sum([vl1[na] * vl1[na] for na in range(len(vl3))])\n",
    "sumb2 = sum([vl3[na] * vl3[na] for na in range(len(vl3))])\n",
    "sumab / math.sqrt(suma2 * sumb2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну конечно же! Надо же пересчитать все вектора, а то размерность пространства изменилась! Со словарями такой проблемы не было."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl1 = faster3.vectorizeAsList(words)\n",
    "vl2 = faster3.vectorizeAsList(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7853540382103844\n",
      "0.7776684489226509\n",
      "CPU times: user 22.5 ms, sys: 0 ns, total: 22.5 ms\n",
      "Wall time: 21.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vl1, vl3))\n",
    "print(faster3.cosineSimilarity(vl2, vl3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956, 15777, 17677, 19267)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl1.count(0), vl2.count(0), vl3.count(0), len(vl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем построить вектора для всего текста \"Марсианина\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words5 = []\n",
    "for i in range(2, 33):\n",
    "    with open(f'data/veyr/index_split_0{i:0>2}.xhtml') as fil:\n",
    "        textMar = fil.read()\n",
    "    words6 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textMar)]\n",
    "    words5 += words6\n",
    "newtext5 = ' '.join(words5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd4 = faster3.vectorizeAsDict(words5)\n",
    "vl4 = faster3.vectorizeAsList(words5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl1 = faster3.vectorizeAsList(words)\n",
    "vl2 = faster3.vectorizeAsList(words3)\n",
    "vl3 = faster3.vectorizeAsList(words4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8039365043485874\n",
      "0.8083884676412758\n",
      "0.8816806000490218\n",
      "CPU times: user 23.6 ms, sys: 663 µs, total: 24.3 ms\n",
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vd1, vd4))\n",
    "print(faster3.cosineSimilarity(vd2, vd4))\n",
    "print(faster3.cosineSimilarity(vd3, vd4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, глава из \"Марсианина\" больше похожа на всё произведение, чем на Толстого. Но общая мера сходства довольно большая. То есть по-хорошему, речь идет в большой степени об одинаковых вещах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8039365043485874\n",
      "0.8083884676412758\n",
      "0.8816806000490218\n",
      "CPU times: user 34.7 ms, sys: 394 µs, total: 35.1 ms\n",
      "Wall time: 33.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vl1, vl4))\n",
    "print(faster3.cosineSimilarity(vl2, vl4))\n",
    "print(faster3.cosineSimilarity(vl3, vl4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18311 3490 1590 8011 22241\n",
      "445508 90841\n"
     ]
    }
   ],
   "source": [
    "print(len(vl1) - vl1.count(0), \n",
    "      len(vl1) - vl2.count(0), \n",
    "      len(vl1) - vl3.count(0), \n",
    "      len(vl1) - vl4.count(0), \n",
    "      len(vl1))\n",
    "print(len(words), len(words5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104178\n",
      "8736\n"
     ]
    }
   ],
   "source": [
    "with open('data/war_and_peace3.txt') as fil:\n",
    "    textWP = fil.read()\n",
    "words6 = [w[0] for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "print(len(words6))\n",
    "faster4 = FasterMorphology2()\n",
    "vd5 = faster4.vectorizeAsDict(words6)\n",
    "vl5 = faster4.vectorizeAsList(words6)\n",
    "print(len(vl5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ещё два слова про производительность\n",
    "\n",
    "Как уже было сказано выше, вычисление косинуса при помощи SciPy будет быстрее. На самом деле почти всё будет быстрее, чем тот код, что написан выше, но мне хотелось сравнимых решений, демонстрирующих внутреннюю структуру метода.\n",
    "\n",
    "А теперь давайте посмотрим как можно сделать быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим случайный массив\n",
    "arr_size = 1000\n",
    "a = np.random.rand(100)\n",
    "ad = {i:a[i] for i in range(100)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 µs ± 874 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Вот так считается часть косинусной меры у нас.\n",
    "s = sum([ad[i]*ad[i] for i in ad.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.06 µs ± 67.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Используем скалярное произведение векторов для того, чтобы посчитать сумму поэлементных произведений.\n",
    "s = np.dot(np.array(list(ad.values())), np.array(list(ad.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48 µs ± 658 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Этот небольшой трюк сократит время расчетов в два раза.\n",
    "a2 = np.array(list(ad.values()))\n",
    "s = np.dot(a2, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785 ns ± 57.5 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = np.dot(a, a) # А если бы мы с самого начала использовали numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.19 µs ± 501 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Используем скалярное произведение векторов для того, чтобы посчитать сумму поэлементных произведений.\n",
    "s = cp.dot(np.array(list(ad.values())), np.array(list(ad.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.87 µs ± 567 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Этот небольшой трюк сократит время расчетов в два раза.\n",
    "a2 = np.array(list(ad.values()))\n",
    "s = cp.dot(a2, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475 ns ± 3.77 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = cp.dot(a, a) # А если бы мы с самого начала использовали numpy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CountVectorizer и TfidfVectorizer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле примерно всё то же самое можно селать при помощи класса CountVectorizer из sklearn.feature_extraction.text. При помощи функции <i>fit\\_transform</i> можно получить разреженное представление матрицы частот слов. Основная проблема состоит в том, что индексы в матрице представляют собой индексы в словаре переданных текстов. Сам словарь хранится в свойстве <i>vocabulary\\_</i> и умеет возвращать индекс по слову (но не наоборот)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t10\n",
      "  (0, 6)\t7\n",
      "  (0, 7)\t3\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 9)\t3\n",
      "20342\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "counter = CountVectorizer()\n",
    "# Просим посчитать частоты слов.\n",
    "res = counter.fit_transform([newtext, newtext3, newtext5])\n",
    "# Разреженное представление счетчика.\n",
    "print(res[0][0,:10])\n",
    "# Можно получить индекс по слову, ...\n",
    "print(counter.vocabulary_.get('левый'))\n",
    "# ... но не наоборот.\n",
    "print(counter.vocabulary_.get(20342))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, CountVectorizer просто выделяет подстроки и ничего не знает про морфологию (ее можно правильно прикрутить, но это хлопотное занятие). Зато он умеет выделять n-граммы (n слов идущих подряд (или даже букв)). Помимо этого, можно попросить выдать все подстроки, создав анализатор. И можно сказать как выделять подстроки при помощи регулярного выражения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeaningfullWords(text):\n",
    "    words = []\n",
    "    tokens = re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', text)\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        if pv[0].tag.POS in ['ADJF', 'NOUN', 'VERB', 'PRTF', 'GRND']:\n",
    "            words.append(pv[0].normal_form)\n",
    "    return words\n",
    "\n",
    "lemmaCounter = CountVectorizer(ngram_range=(1,3), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "c = [' '.join(getMeaningfullWords(newtext)),\n",
    "     ' '.join(getMeaningfullWords(newtext3)),\n",
    "     ' '.join(getMeaningfullWords(newtext5))]\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "res1 = analyze(c[0])\n",
    "res2 = lemmaCounter.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лев', 'николаевич', 'толстой', 'война', 'мир', 'тот', 'часть', 'первый', 'быть', 'поместье']\n"
     ]
    }
   ],
   "source": [
    "print(res1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем другой показатель для подсчета важности слов в тексте - $TF*IDF$. Здесь $TF$ - Term Frequency, частота термина в документе, а $IDF$ - Inverted Document Frequency, обратная частота термина в коллекции (количество документов, в которых встречается данный термин).\n",
    "\n",
    "Идея метрики очень проста. Если слово встречается почти во всех документах - его различительная сила очень мала и само слово не является важным. Если слово часто встречается в данном документе, то оно являетсяя важным для него.\n",
    "\n",
    "Метрика считается на коллекции документов для каждого слова, каждого документа. Для расчета меры можно использовать `TfidfVectorizer`, который работает так же как `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaCounter = TfidfVectorizer(ngram_range=(1,3), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "c = [' '.join(getMeaningfullWords(newtext)),\n",
    "     ' '.join(getMeaningfullWords(newtext3)),\n",
    "     ' '.join(getMeaningfullWords(newtext5))]\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "res1 = analyze(c[0])\n",
    "res2 = lemmaCounter.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.00011569805879228521\n",
      "  (0, 5)\t0.00011569805879228521\n",
      "  (0, 1)\t0.00011569805879228521\n",
      "  (0, 4)\t0.00011569805879228521\n",
      "  (0, 9)\t0.00023139611758457042\n",
      "  (0, 0)\t0.00011569805879228521\n",
      "  (0, 3)\t0.00011569805879228521\n"
     ]
    }
   ],
   "source": [
    "print(res2[0][0,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем посмотреть на близость глав \"Марсианина\" при помощи косинусной меры по частотам слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsM = []\n",
    "for i in range(2, 33):\n",
    "    with open(f'data/veyr/index_split_0{i:0>2}.xhtml') as fil:\n",
    "        textWP = fil.read()\n",
    "    words6 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "    wordsM.append(words6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vects = []\n",
    "\n",
    "for words in wordsM:\n",
    "    m_vects.append(faster3.vectorizeAsDict(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9420221325678382 22 25\n"
     ]
    }
   ],
   "source": [
    "nearest = -1\n",
    "txt1 = -1\n",
    "txt2 = -1\n",
    "for i, vct1 in enumerate(m_vects):\n",
    "    for j, vct2 in enumerate(m_vects):\n",
    "        if vct1 is vct2:\n",
    "            continue\n",
    "        cc = faster3.cosineSimilarity(vct1, vct2)\n",
    "        if cc > nearest:\n",
    "            nearest = cc\n",
    "            txt1 = i\n",
    "            txt2 = j\n",
    "print(nearest, txt1+2, txt2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь возьмем косинусную меру сходства по результатам TF*IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "textM = []\n",
    "for i in range(2, 33):\n",
    "    with open(f'data/veyr/index_split_0{i:0>2}.xhtml') as fil:\n",
    "        textM.append(fil.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaCounter = CountVectorizer(ngram_range=(1,3), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "#lemmaCounter=TfidfVectorizer(ngram_range=(1,3), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "#res1=analyze(c[0])\n",
    "res2 = lemmaCounter.fit_transform(textM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85149324]] 15 21\n"
     ]
    }
   ],
   "source": [
    "nearest = -1\n",
    "txt1 = -1\n",
    "txt2 = -1\n",
    "for i, vct1 in enumerate(res2):\n",
    "    for j, vct2 in enumerate(res2):\n",
    "        if i==j:\n",
    "            continue\n",
    "        cc = cosine_similarity(vct1, vct2)\n",
    "        if cc > nearest:\n",
    "            nearest = cc\n",
    "            txt1 = i\n",
    "            txt2 = j\n",
    "print(nearest, txt1+2, txt2+2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
